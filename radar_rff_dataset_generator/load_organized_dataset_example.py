#!/usr/bin/env python
"""
åŒé‡ç»„ç»‡æ•°æ®é›†åŠ è½½ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨é‡ç»„åçš„æ•°æ®é›†è¿›è¡Œä¸åŒçš„ç ”ç©¶ä»»åŠ¡
"""

import h5py
import numpy as np
from pathlib import Path


def example_1_load_by_device():
    """
    ç¤ºä¾‹1: æŒ‰è®¾å¤‡åŠ è½½ - è®¾å¤‡æŒ‡çº¹è¯†åˆ«ä»»åŠ¡
    
    åœºæ™¯ï¼šç ”ç©¶è®¾å¤‡5åœ¨æ‰€æœ‰è°ƒåˆ¶æ–¹å¼ä¸‹çš„RFFç‰¹å¾
    """
    print("\n" + "="*70)
    print("ğŸ“± ç¤ºä¾‹1: æŒ‰è®¾å¤‡åŠ è½½æ•°æ®")
    print("="*70)
    
    device_id = 5
    device_file = f'radar_rff_dataset_organized/by_device/device_{device_id:02d}_Radar_Device_{device_id+1:02d}.h5'
    
    with h5py.File(device_file, 'r') as f:
        # è·å–åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“Š è®¾å¤‡ä¿¡æ¯:")
        print(f"   è®¾å¤‡ID: {f.attrs['device_id']}")
        print(f"   è®¾å¤‡åç§°: {f.attrs['device_name']}")
        print(f"   æ ·æœ¬æ€»æ•°: {f.attrs['num_samples']:,}")
        print(f"   è°ƒåˆ¶æ•°é‡: {f.attrs['num_modulations']}")
        print(f"   æ¯ç§è°ƒåˆ¶æ ·æœ¬æ•°: {f.attrs['samples_per_modulation']:,}")
        
        # åŠ è½½æ•°æ®
        iq_data = f['iq_data'][:]
        modulation_labels = f['modulation_labels'][:]
        
        print(f"\nğŸ“¦ æ•°æ®å½¢çŠ¶:")
        print(f"   IQæ•°æ®: {iq_data.shape}")
        print(f"   è°ƒåˆ¶æ ‡ç­¾: {modulation_labels.shape}")
        
        # ç»Ÿè®¡æ¯ç§è°ƒåˆ¶çš„æ ·æœ¬æ•°
        print(f"\nğŸ“¡ å„è°ƒåˆ¶æ–¹å¼æ ·æœ¬åˆ†å¸ƒ:")
        unique_mods, counts = np.unique(modulation_labels, return_counts=True)
        for mod_id, count in zip(unique_mods[:5], counts[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"   è°ƒåˆ¶ {mod_id:2d}: {count:,} æ ·æœ¬")
        print(f"   ... (å…± {len(unique_mods)} ç§è°ƒåˆ¶)")
        
        # æå–ç‰¹å®šè°ƒåˆ¶çš„æ•°æ® (ä¾‹å¦‚QPSK, ID=5)
        qpsk_mask = (modulation_labels == 5)
        qpsk_samples = iq_data[qpsk_mask]
        print(f"\nğŸ¯ æå–QPSKè°ƒåˆ¶æ ·æœ¬:")
        print(f"   æ ·æœ¬æ•°: {len(qpsk_samples):,}")
        print(f"   å½¢çŠ¶: {qpsk_samples.shape}")


def example_2_load_by_modulation():
    """
    ç¤ºä¾‹2: æŒ‰è°ƒåˆ¶åŠ è½½ - è°ƒåˆ¶è¯†åˆ«ä»»åŠ¡
    
    åœºæ™¯ï¼šç ”ç©¶QPSKè°ƒåˆ¶åœ¨æ‰€æœ‰è®¾å¤‡ä¸Šçš„è¡¨ç°
    """
    print("\n" + "="*70)
    print("ğŸ“¡ ç¤ºä¾‹2: æŒ‰è°ƒåˆ¶åŠ è½½æ•°æ®")
    print("="*70)
    
    modulation_id = 5  # QPSK
    modulation_name = 'qpsk'
    mod_file = f'radar_rff_dataset_organized/by_modulation/modulation_{modulation_id:02d}_{modulation_name}.h5'
    
    with h5py.File(mod_file, 'r') as f:
        # è·å–åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“Š è°ƒåˆ¶ä¿¡æ¯:")
        print(f"   è°ƒåˆ¶ID: {f.attrs['modulation_id']}")
        print(f"   è°ƒåˆ¶åç§°: {f.attrs['modulation_name']}")
        print(f"   æ ·æœ¬æ€»æ•°: {f.attrs['num_samples']:,}")
        print(f"   è®¾å¤‡æ•°é‡: {f.attrs['num_devices']}")
        print(f"   æ¯ä¸ªè®¾å¤‡æ ·æœ¬æ•°: {f.attrs['samples_per_device']:,}")
        
        # åŠ è½½æ•°æ®
        iq_data = f['iq_data'][:]
        device_labels = f['device_labels'][:]
        
        print(f"\nğŸ“¦ æ•°æ®å½¢çŠ¶:")
        print(f"   IQæ•°æ®: {iq_data.shape}")
        print(f"   è®¾å¤‡æ ‡ç­¾: {device_labels.shape}")
        
        # ç»Ÿè®¡æ¯ä¸ªè®¾å¤‡çš„æ ·æœ¬æ•°
        print(f"\nğŸ“± å„è®¾å¤‡æ ·æœ¬åˆ†å¸ƒ:")
        unique_devs, counts = np.unique(device_labels, return_counts=True)
        for dev_id, count in zip(unique_devs[:5], counts[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"   è®¾å¤‡ {dev_id:2d}: {count:,} æ ·æœ¬")
        print(f"   ... (å…± {len(unique_devs)} ä¸ªè®¾å¤‡)")
        
        # æå–ç‰¹å®šè®¾å¤‡çš„æ•°æ® (ä¾‹å¦‚è®¾å¤‡5)
        device_5_mask = (device_labels == 5)
        device_5_samples = iq_data[device_5_mask]
        print(f"\nğŸ¯ æå–è®¾å¤‡5çš„æ ·æœ¬:")
        print(f"   æ ·æœ¬æ•°: {len(device_5_samples):,}")
        print(f"   å½¢çŠ¶: {device_5_samples.shape}")


def example_3_batch_loading():
    """
    ç¤ºä¾‹3: æ‰¹é‡åŠ è½½ - è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹
    
    åœºæ™¯ï¼šåŠ è½½å¤šä¸ªè®¾å¤‡çš„æ•°æ®ç”¨äºè®­ç»ƒ
    """
    print("\n" + "="*70)
    print("ğŸ”„ ç¤ºä¾‹3: æ‰¹é‡åŠ è½½å¤šä¸ªè®¾å¤‡")
    print("="*70)
    
    # åŠ è½½è®¾å¤‡0-4çš„æ‰€æœ‰æ•°æ®
    device_ids = range(5)
    all_data = []
    all_device_labels = []
    all_mod_labels = []
    
    print("\nğŸ“¥ æ­£åœ¨åŠ è½½è®¾å¤‡...")
    for device_id in device_ids:
        device_file = f'radar_rff_dataset_organized/by_device/device_{device_id:02d}_Radar_Device_{device_id+1:02d}.h5'
        
        with h5py.File(device_file, 'r') as f:
            iq_data = f['iq_data'][:]
            mod_labels = f['modulation_labels'][:]
            
            all_data.append(iq_data)
            all_device_labels.append(np.full(len(iq_data), device_id))
            all_mod_labels.append(mod_labels)
            
            print(f"   âœ“ è®¾å¤‡ {device_id}: {len(iq_data):,} æ ·æœ¬")
    
    # åˆå¹¶æ•°æ®
    all_data = np.concatenate(all_data, axis=0)
    all_device_labels = np.concatenate(all_device_labels, axis=0)
    all_mod_labels = np.concatenate(all_mod_labels, axis=0)
    
    print(f"\nğŸ“Š åˆå¹¶åçš„æ•°æ®:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(all_data):,}")
    print(f"   æ•°æ®å½¢çŠ¶: {all_data.shape}")
    print(f"   è®¾å¤‡æ ‡ç­¾å½¢çŠ¶: {all_device_labels.shape}")
    print(f"   è°ƒåˆ¶æ ‡ç­¾å½¢çŠ¶: {all_mod_labels.shape}")
    
    # æ•°æ®é›†åˆ’åˆ†ç¤ºä¾‹
    num_samples = len(all_data)
    train_size = int(0.8 * num_samples)
    
    # æ‰“ä¹±æ•°æ®
    indices = np.random.permutation(num_samples)
    train_indices = indices[:train_size]
    test_indices = indices[train_size:]
    
    print(f"\nğŸ“ æ•°æ®é›†åˆ’åˆ†:")
    print(f"   è®­ç»ƒé›†: {len(train_indices):,} æ ·æœ¬ ({len(train_indices)/num_samples*100:.1f}%)")
    print(f"   æµ‹è¯•é›†: {len(test_indices):,} æ ·æœ¬ ({len(test_indices)/num_samples*100:.1f}%)")


def example_4_cross_modulation_generalization():
    """
    ç¤ºä¾‹4: è·¨è°ƒåˆ¶æ³›åŒ–æµ‹è¯•
    
    åœºæ™¯ï¼šåœ¨æŸäº›è°ƒåˆ¶ä¸Šè®­ç»ƒï¼Œåœ¨å…¶ä»–è°ƒåˆ¶ä¸Šæµ‹è¯•
    """
    print("\n" + "="*70)
    print("ğŸ”¬ ç¤ºä¾‹4: è·¨è°ƒåˆ¶æ³›åŒ–æµ‹è¯•")
    print("="*70)
    
    device_id = 5
    device_file = f'radar_rff_dataset_organized/by_device/device_{device_id:02d}_Radar_Device_{device_id+1:02d}.h5'
    
    with h5py.File(device_file, 'r') as f:
        iq_data = f['iq_data'][:]
        modulation_labels = f['modulation_labels'][:]
        
        # è®­ç»ƒè°ƒåˆ¶: QAMç³»åˆ— (0-3)
        train_mods = [0, 1, 2, 3]
        train_mask = np.isin(modulation_labels, train_mods)
        train_data = iq_data[train_mask]
        
        # æµ‹è¯•è°ƒåˆ¶: PSKç³»åˆ— (4-8)
        test_mods = [4, 5, 6, 7, 8]
        test_mask = np.isin(modulation_labels, test_mods)
        test_data = iq_data[test_mask]
        
        print(f"\nğŸ“š è®­ç»ƒé›† (QAMç³»åˆ—):")
        print(f"   è°ƒåˆ¶æ–¹å¼: {train_mods}")
        print(f"   æ ·æœ¬æ•°: {len(train_data):,}")
        
        print(f"\nğŸ§ª æµ‹è¯•é›† (PSKç³»åˆ—):")
        print(f"   è°ƒåˆ¶æ–¹å¼: {test_mods}")
        print(f"   æ ·æœ¬æ•°: {len(test_data):,}")
        
        print(f"\nğŸ’¡ ç”¨é€”: æµ‹è¯•æ¨¡å‹åœ¨æœªè§è¿‡çš„è°ƒåˆ¶æ–¹å¼ä¸Šçš„æ³›åŒ–èƒ½åŠ›")


def example_5_pytorch_dataloader():
    """
    ç¤ºä¾‹5: PyTorch DataLoaderé›†æˆ
    
    åœºæ™¯ï¼šåˆ›å»ºPyTorchæ•°æ®åŠ è½½å™¨ç”¨äºè®­ç»ƒ
    """
    print("\n" + "="*70)
    print("ğŸ”¥ ç¤ºä¾‹5: PyTorch DataLoaderé›†æˆ")
    print("="*70)
    
    print("\nğŸ“ PyTorch Datasetç±»ç¤ºä¾‹ä»£ç :")
    print("""
from torch.utils.data import Dataset, DataLoader
import h5py
import torch

class RadarRFFDataset(Dataset):
    def __init__(self, device_ids, base_dir='radar_rff_dataset_organized/by_device'):
        self.data = []
        self.device_labels = []
        self.mod_labels = []
        
        # åŠ è½½æŒ‡å®šè®¾å¤‡çš„æ•°æ®
        for device_id in device_ids:
            filename = f'{base_dir}/device_{device_id:02d}_Radar_Device_{device_id+1:02d}.h5'
            with h5py.File(filename, 'r') as f:
                self.data.append(f['iq_data'][:])
                self.mod_labels.append(f['modulation_labels'][:])
                self.device_labels.append(np.full(len(f['iq_data']), device_id))
        
        self.data = np.concatenate(self.data, axis=0)
        self.device_labels = np.concatenate(self.device_labels, axis=0)
        self.mod_labels = np.concatenate(self.mod_labels, axis=0)
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        # è¿”å›IQæ•°æ®å’Œæ ‡ç­¾
        iq_sample = torch.from_numpy(self.data[idx]).float()
        device_label = torch.tensor(self.device_labels[idx]).long()
        mod_label = torch.tensor(self.mod_labels[idx]).long()
        
        return iq_sample, device_label, mod_label

# ä½¿ç”¨ç¤ºä¾‹
train_dataset = RadarRFFDataset(device_ids=[0, 1, 2, 3, 4])
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)

# è®­ç»ƒå¾ªç¯
for batch_idx, (iq_data, device_labels, mod_labels) in enumerate(train_loader):
    # iq_data shape: [64, 2, 2049]
    # device_labels shape: [64]
    # mod_labels shape: [64]
    
    # ä½ çš„è®­ç»ƒä»£ç ...
    pass
""")


def example_6_dataset_statistics():
    """
    ç¤ºä¾‹6: æ•°æ®é›†ç»Ÿè®¡åˆ†æ
    
    åœºæ™¯ï¼šåˆ†ææ•°æ®é›†çš„æ•´ä½“ç‰¹å¾
    """
    print("\n" + "="*70)
    print("ğŸ“ˆ ç¤ºä¾‹6: æ•°æ®é›†ç»Ÿè®¡åˆ†æ")
    print("="*70)
    
    # éšæœºé€‰æ‹©ä¸€ä¸ªè®¾å¤‡æ–‡ä»¶è¿›è¡Œåˆ†æ
    device_id = 0
    device_file = f'radar_rff_dataset_organized/by_device/device_{device_id:02d}_Radar_Device_{device_id+1:02d}.h5'
    
    with h5py.File(device_file, 'r') as f:
        # è¯»å–å°‘é‡æ ·æœ¬è¿›è¡Œç»Ÿè®¡ (é¿å…å†…å­˜æº¢å‡º)
        sample_size = 1000
        iq_data = f['iq_data'][:sample_size]
        
        # è®¡ç®—ç»Ÿè®¡é‡
        i_channel = iq_data[:, 0, :]  # Ié€šé“
        q_channel = iq_data[:, 1, :]  # Qé€šé“
        
        print(f"\nğŸ“Š ä¿¡å·ç»Ÿè®¡ (åŸºäº{sample_size}ä¸ªæ ·æœ¬):")
        print(f"\n   Ié€šé“:")
        print(f"      å‡å€¼: {i_channel.mean():.6f}")
        print(f"      æ ‡å‡†å·®: {i_channel.std():.6f}")
        print(f"      æœ€å°å€¼: {i_channel.min():.6f}")
        print(f"      æœ€å¤§å€¼: {i_channel.max():.6f}")
        
        print(f"\n   Qé€šé“:")
        print(f"      å‡å€¼: {q_channel.mean():.6f}")
        print(f"      æ ‡å‡†å·®: {q_channel.std():.6f}")
        print(f"      æœ€å°å€¼: {q_channel.min():.6f}")
        print(f"      æœ€å¤§å€¼: {q_channel.max():.6f}")
        
        # è®¡ç®—ä¿¡å·åŠŸç‡
        power = i_channel**2 + q_channel**2
        print(f"\n   ä¿¡å·åŠŸç‡:")
        print(f"      å¹³å‡åŠŸç‡: {power.mean():.6f}")
        print(f"      åŠŸç‡æ ‡å‡†å·®: {power.std():.6f}")


def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("\n" + "="*70)
    print("ğŸ¯ åŒé‡ç»„ç»‡æ•°æ®é›†åŠ è½½ç¤ºä¾‹")
    print("="*70)
    print("\næœ¬è„šæœ¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨é‡ç»„åçš„æ•°æ®é›†")
    print("è¯·ç¡®ä¿å·²ç»è¿è¡Œè¿‡ organize_dataset.py ç”Ÿæˆäº†é‡ç»„æ•°æ®")
    
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    base_dir = Path('radar_rff_dataset_organized')
    if not base_dir.exists():
        print(f"\nâŒ é”™è¯¯: æ‰¾ä¸åˆ°é‡ç»„åçš„æ•°æ®é›†ç›®å½•: {base_dir}")
        print("   è¯·å…ˆè¿è¡Œ: python organize_dataset.py")
        return
    
    try:
        # è¿è¡Œç¤ºä¾‹ (æ³¨é‡Šæ‰ä¸éœ€è¦çš„ç¤ºä¾‹)
        example_1_load_by_device()
        example_2_load_by_modulation()
        example_3_batch_loading()
        example_4_cross_modulation_generalization()
        example_5_pytorch_dataloader()
        example_6_dataset_statistics()
        
    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("   è¯·ç¡®ä¿æ•°æ®é›†å·²ç»ç”Ÿæˆå¹¶é‡ç»„")
    
    print("\n" + "="*70)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("="*70)


if __name__ == '__main__':
    main()

